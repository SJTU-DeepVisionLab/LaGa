{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6fa672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cenjiazhong/.conda/envs/laga/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import pytorch3d.ops\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import cv2\n",
    "\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from scene import Scene, GaussianModel, FeatureGaussianModel\n",
    "from gaussian_renderer import render, render_contrastive_feature\n",
    "from tqdm import tqdm\n",
    "from utils.sh_utils import SH2RGB\n",
    "\n",
    "def get_combined_args(parser : ArgumentParser, model_path, target_cfg_file = None):\n",
    "    cmdlne_string = ['--model_path', model_path]\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "    \n",
    "    if target_cfg_file is None:\n",
    "        if args_cmdline.target == 'seg':\n",
    "            target_cfg_file = \"seg_cfg_args\"\n",
    "        elif args_cmdline.target == 'scene' or args_cmdline.target == 'xyz':\n",
    "            target_cfg_file = \"cfg_args\"\n",
    "        elif args_cmdline.target == 'feature' or args_cmdline.target == 'coarse_seg_everything' or args_cmdline.target == 'contrastive_feature' :\n",
    "            target_cfg_file = \"feature_cfg_args\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(model_path, target_cfg_file)\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file found: {}\".format(cfgfilepath))\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "\n",
    "    return Namespace(**merged_dict)\n",
    "\n",
    "def load_point_colors_from_pcd(num_points, path):\n",
    "    plydata = PlyData.read(path)\n",
    "\n",
    "    features_dc = np.zeros((num_points, 3))\n",
    "    features_dc[:, 0] = np.asarray(plydata.elements[0][\"f_dc_0\"])\n",
    "    features_dc[:, 1] = np.asarray(plydata.elements[0][\"f_dc_1\"])\n",
    "    features_dc[:, 2] = np.asarray(plydata.elements[0][\"f_dc_2\"])\n",
    "\n",
    "    colors = SH2RGB(features_dc)\n",
    "\n",
    "    # N, 3\n",
    "    return torch.clamp(torch.from_numpy(colors).squeeze().cuda(), 0.0, 1.0) * 255.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384f5ab",
   "metadata": {},
   "source": [
    "# Model Setting and Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c4629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FEATURE_DIM = 32\n",
    "SCALE_AWARE_DIM = 24\n",
    "\n",
    "\n",
    "scale_aware_dim = SCALE_AWARE_DIM\n",
    "fixed_scale_gate = torch.tensor([[1 for j in range(32 - scale_aware_dim + i)] + [0 for k in range(scale_aware_dim - i)] for i in range(scale_aware_dim+1)]).cuda()\n",
    "    \n",
    "DATA_ROOT = './data/nerf_llff_data_for_3dgs/'\n",
    "# MODEL_PATH = './output/3dovs-bed/'\n",
    "\n",
    "MODEL_PATH = '../saga2/output/lerfovs-figurines-mini/' # 31000\n",
    "# MODEL_PATH = './output/scannet-scene0000_00_og/' # 30000\n",
    "# MODEL_PATH = './output/scannet-scene0645_00/' # 30000\n",
    "# MODEL_PATH = './output/3dovs-room/' # 30000\n",
    "# MODEL_PATH = './output/360-room/' # 31000\n",
    "\n",
    "NUM_LVL = 1 if ('scannet' in MODEL_PATH and 'og' in MODEL_PATH) or '3dovs' in MODEL_PATH else 3\n",
    "FEATURE_GAUSSIAN_ITERATION = 31000\n",
    "\n",
    "FEATURE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/contrastive_feature_point_cloud.ply')\n",
    "SCENE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scene_point_cloud.ply')\n",
    "\n",
    "CLIP_PATH = '../sagav2/clip_ckpt/ViT-B-16-laion2b_s34b_b88k.bin'\n",
    "\n",
    "MULTI_LVL_DIM = [16,8,8] if NUM_LVL == 3 else [32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b4b63",
   "metadata": {},
   "source": [
    "# Data and Model Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316ebd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for config file in ../saga2/output/lerfovs-figurines-mini/cfg_args\n",
      "Config file found: ../saga2/output/lerfovs-figurines-mini/cfg_args\n",
      "Loading trained model at iteration 30000, 31000\n",
      "Allow Camera Principle Point Shift: False\n",
      "Reading camera 299/299\n",
      "Loading Training Cameras\n",
      "Loading Test Cameras\n"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument(\"--iteration\", default=-1, type=int)\n",
    "parser.add_argument(\"--skip_train\", action=\"store_true\")\n",
    "parser.add_argument(\"--skip_test\", action=\"store_true\")\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\")\n",
    "parser.add_argument(\"--segment\", action=\"store_true\")\n",
    "parser.add_argument('--target', default='scene', const='scene', nargs='?', choices=['scene', 'seg', 'feature', 'coarse_seg_everything', 'contrastive_feature', 'xyz'])\n",
    "parser.add_argument('--idx', default=0, type=int)\n",
    "parser.add_argument('--precomputed_mask', default=None, type=str)\n",
    "\n",
    "args = get_combined_args(parser, MODEL_PATH)\n",
    "\n",
    "dataset = model.extract(args)\n",
    "dataset.need_features = True\n",
    "dataset.need_masks = True\n",
    "\n",
    "scene_gaussians = GaussianModel(dataset.sh_degree)\n",
    "\n",
    "feature_gaussians = FeatureGaussianModel(FEATURE_DIM)\n",
    "scene = Scene(dataset, scene_gaussians, feature_gaussians, load_iteration=-1, feature_load_iteration=FEATURE_GAUSSIAN_ITERATION, shuffle=False, mode='eval', target='contrastive_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fbae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed decomposition results and semantic descriptors\n",
    "multi_lvl_cluster_features = torch.load(os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_cluster_features.pth'))\n",
    "multi_lvl_cluster_feature_weights = torch.load(os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_cluster_feature_weights.pth'))\n",
    "\n",
    "try:\n",
    "    multi_lvl_cluster_feature_weights_only_direction = torch.load(os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_cluster_feature_weights_only_direction.pth'))\n",
    "except:\n",
    "    print(\"No direction-only weights found!\")\n",
    "\n",
    "multi_lvl_seg_scores = torch.load(os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_seg_scores.pth'))\n",
    "\n",
    "# max num of semantic descriptors per object\n",
    "num_per_cluster_features = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d85228",
   "metadata": {},
   "source": [
    "# Load Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07ca2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 299 views in the dataset.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cameras = scene.getTrainCameras()\n",
    "print(\"There are\",len(cameras),\"views in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a887941",
   "metadata": {},
   "source": [
    "# 3D Scene Decomposition (Sec. 5.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfa6585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/299 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:10<00:00, 28.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
      "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
      "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
      "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
      "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214\n",
      " 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232\n",
      " 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250\n",
      " 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268\n",
      " 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286\n",
      " 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448\n",
      " 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466\n",
      " 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484\n",
      " 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502\n",
      " 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520\n",
      " 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538\n",
      " 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556\n",
      " 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574\n",
      " 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592\n",
      " 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610\n",
      " 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628\n",
      " 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643]\n",
      "[ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
      "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
      "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
      "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
      "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 197 198 199 200 201 202 203]\n",
      "[-1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n",
      " 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46\n",
      " 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "MASK_SIZE_THRESH = 400\n",
    "\n",
    "import hdbscan\n",
    "with torch.no_grad():\n",
    "    multi_lvl_prototypes = [[],[],[]]\n",
    "    multi_lvl_mask_filter = [[],[],[]]\n",
    "\n",
    "    for tview in tqdm(scene.getTrainCameras()):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        view = deepcopy(tview)\n",
    "\n",
    "        view.feature_height, view.feature_width = view.image_height, view.image_width\n",
    "\n",
    "        bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "        background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "        rendered_feature = render_contrastive_feature(view, feature_gaussians, pipeline.extract(args), background, norm_point_features=True, multi_lvl_norm=True, multi_lvl_dim=MULTI_LVL_DIM, smooth_type = None)['render']\n",
    "        rendered_feature = rendered_feature.permute([1,2,0])\n",
    "        sam_masks = view.original_masks.cuda()\n",
    "\n",
    "        multi_lvl_masks_before_filt = []\n",
    "        multi_lvl_masks = []\n",
    "\n",
    "        sam_masks[sam_masks == -1] = -1000\n",
    "        for lvl in range(1,NUM_LVL+1) if NUM_LVL > 1 else [0]:\n",
    "            tmp_masks = sam_masks[lvl].clone()\n",
    "\n",
    "            # for garden, disable it in other scenes (Low resolution for reducing GPU memory consumption)\n",
    "            # tmp_masks = torch.nn.functional.interpolate(tmp_masks.unsqueeze(0).unsqueeze(0), (tmp_masks.shape[-2] //2,tmp_masks.shape[-1] //2), mode='nearest').squeeze()\n",
    "        \n",
    "            smallest_index = sam_masks[lvl-1].int().max()+1 if lvl > 0 else 0 \n",
    "            tmp_masks -= smallest_index-1\n",
    "            tmp_masks[tmp_masks < 0] = 0\n",
    "\n",
    "            tmp_lvl_masks = torch.nn.functional.one_hot(tmp_masks.long(), num_classes=tmp_masks.max().int().item()+1)[:,:,1:].float()\n",
    "\n",
    "            multi_lvl_masks_before_filt.append(tmp_lvl_masks)\n",
    "        \n",
    "        for lvl in range(NUM_LVL):\n",
    "            tmp_mask = multi_lvl_masks_before_filt[lvl]\n",
    "\n",
    "            mask_non_zero_count = tmp_mask.sum(dim = (0,1))\n",
    "\n",
    "            # remove some extremely small masks which may have ambiguous semantic meaning\n",
    "            fi = mask_non_zero_count > MASK_SIZE_THRESH\n",
    "\n",
    "            tmp_mask = tmp_mask[:,:,fi]\n",
    "\n",
    "            multi_lvl_masks.append(tmp_mask)\n",
    "\n",
    "            multi_lvl_mask_filter[lvl].append(fi)\n",
    "            \n",
    "        normed_rendered_feature = []\n",
    "        pre_dim = 0\n",
    "        for dim in MULTI_LVL_DIM:\n",
    "            normed_rendered_feature.append(torch.nn.functional.normalize(rendered_feature[:,:,pre_dim:pre_dim+dim], dim = -1))\n",
    "            pre_dim += dim\n",
    "        normed_rendered_feature = torch.cat(normed_rendered_feature, dim = -1)\n",
    "\n",
    "        pre_dim = 0\n",
    "        multi_lvl_features = []\n",
    "        for i in range(NUM_LVL):\n",
    "            multi_lvl_features.insert(0, torch.nn.functional.normalize(normed_rendered_feature[:,:,:MULTI_LVL_DIM[i]+pre_dim], dim = -1))\n",
    "            pre_dim += MULTI_LVL_DIM[i]\n",
    "\n",
    "\n",
    "        for lvl in range(NUM_LVL):\n",
    "            if multi_lvl_features[lvl].shape[:2] != multi_lvl_masks[lvl].shape[:2]:\n",
    "                multi_lvl_features[lvl] = torch.nn.functional.interpolate(multi_lvl_features[lvl].permute([2,0,1]).unsqueeze(0), multi_lvl_masks[lvl].shape[:2], mode = 'bilinear').squeeze(0).permute([1,2,0])\n",
    "            \n",
    "            multi_lvl_prototypes[lvl].append(\n",
    "                torch.nn.functional.normalize(torch.einsum('hwc,hwf->fc', multi_lvl_features[lvl], multi_lvl_masks[lvl]), dim=-1, p=2)\n",
    "            )\n",
    "\n",
    "    multi_lvl_prototypes = [torch.cat(multi_lvl_prototypes[lvl], 0) for lvl in range(NUM_LVL)]\n",
    "    multi_lvl_mask_filter = [torch.cat(multi_lvl_mask_filter[lvl], 0) for lvl in range(NUM_LVL)]\n",
    "\n",
    "    multi_lvl_cluster_centers = []\n",
    "    multi_lvl_cluster_labels = []\n",
    "    for lvl in range(NUM_LVL):\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=5, cluster_selection_epsilon = 0.1 * (lvl+1))\n",
    "\n",
    "        cluster_labels = clusterer.fit_predict(multi_lvl_prototypes[lvl].detach().cpu().numpy())\n",
    "        print(np.unique(cluster_labels))\n",
    "\n",
    "        existing_center = 0\n",
    "\n",
    "        cluster_centers = torch.zeros(len(np.unique(cluster_labels[cluster_labels != -1])), multi_lvl_prototypes[lvl].shape[-1]).cuda()\n",
    "        \n",
    "        for i in np.unique(cluster_labels):\n",
    "            if i == -1:\n",
    "                continue\n",
    "\n",
    "            cluster_centers[i] = torch.nn.functional.normalize(multi_lvl_prototypes[lvl][cluster_labels == i].mean(dim = 0), dim = -1)\n",
    "\n",
    "        multi_lvl_cluster_centers.append(cluster_centers)\n",
    "        multi_lvl_cluster_labels.append(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f543f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multi-level cluster centroids to compute segmentation scores\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    point_features = feature_gaussians.get_point_features\n",
    "\n",
    "    normed_point_features = []\n",
    "    pre_dim = 0\n",
    "    for dim in MULTI_LVL_DIM:\n",
    "        normed_point_features.append(torch.nn.functional.normalize(point_features[:,pre_dim:pre_dim+dim], dim = -1))\n",
    "        pre_dim += dim\n",
    "    normed_point_features = torch.cat(normed_point_features, dim = -1)\n",
    "    \n",
    "\n",
    "    pre_dim = 0\n",
    "    multi_lvl_point_features = []\n",
    "    for i in range(NUM_LVL):\n",
    "        multi_lvl_point_features.insert(0, torch.nn.functional.normalize(normed_point_features[:,:MULTI_LVL_DIM[i]+pre_dim], dim = -1))\n",
    "        pre_dim += MULTI_LVL_DIM[i]\n",
    "\n",
    "\n",
    "    multi_lvl_seg_scores = []\n",
    "\n",
    "    for lvl in range(NUM_LVL):\n",
    "        selected_feature = multi_lvl_point_features[lvl]\n",
    "        cluster_centers = multi_lvl_cluster_centers[lvl]\n",
    "\n",
    "        seg_score = torch.einsum('nc,bc->bn', cluster_centers.cuda(), selected_feature.cuda())\n",
    "\n",
    "        multi_lvl_seg_scores.append(seg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d45e5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All semantic features extracted by CLIP\n",
    "multi_lvl_flatten_features = []\n",
    "with torch.no_grad():\n",
    "    for lvl in range(NUM_LVL):\n",
    "        flatten_features = []\n",
    "        for view in scene.getTrainCameras():\n",
    "            torch.cuda.empty_cache()\n",
    "            f = view.original_features.view(-1, 512)\n",
    "\n",
    "            lvl_to_mask_id = []\n",
    "            lastone = -1\n",
    "            # NUM_LVL == 1, scannet from opengaussian\n",
    "            for i in range(0, 4) if NUM_LVL == 3 else range(0, 1):\n",
    "                tm = view.original_masks[i]\n",
    "                curone = tm.max().int().item()\n",
    "                lvl_to_mask_id.append([lastone+1, curone])\n",
    "                lastone = curone\n",
    "            \n",
    "            if NUM_LVL > 1:\n",
    "                interval = lvl_to_mask_id[lvl + 1]\n",
    "                f = f[interval[0]:interval[1]+1,:]\n",
    "            flatten_features.append(f)\n",
    "\n",
    "        multi_lvl_flatten_features.append(torch.cat(flatten_features, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e7d0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645/645 [00:00<00:00, 14975.35it/s]\n",
      "100%|██████████| 205/205 [00:00<00:00, 17439.05it/s]\n",
      "100%|██████████| 69/69 [00:00<00:00, 16511.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3D object to 2D mask mapper. This step builds co-relation among 2D masks (CLIP semantics) for each 3D object\n",
    "multi_lvl_cluster_to_masks = [[],[],[]]\n",
    "for lvl in range(NUM_LVL):\n",
    "    # multi_lvl_cluster_labels\n",
    "    for clus in tqdm(np.unique(multi_lvl_cluster_labels[lvl])):\n",
    "        if clus == -1:\n",
    "            continue\n",
    "        multi_lvl_cluster_to_masks[lvl].append(torch.tensor(multi_lvl_cluster_labels[lvl] == clus).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404afbd",
   "metadata": {},
   "source": [
    "# View-Aggreated Semantic Representation (Sec. 5.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d030b43",
   "metadata": {},
   "source": [
    "In the following four sections (Dynamic K-means, Dynamic K-Means with in loop denoise, Naive K-Means, Average Pooling), you only need to choose one to run. In default, we use dynamic K-means. The other three are for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ff53a",
   "metadata": {},
   "source": [
    "## Cross-View Descriptor Extraction: Dynamic K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abbbc77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26005 torch.Size([26005, 512])\n",
      "13485 torch.Size([13485, 512])\n",
      "6684 torch.Size([6684, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 644/644 [05:38<00:00,  1.90it/s]\n",
      "100%|██████████| 204/204 [02:21<00:00,  1.44it/s]\n",
      "100%|██████████| 68/68 [01:02<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# K-Means with dynamic K selection\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "def dynamic_k_selection(clus_feats, min_clusters=1, max_clusters=20, eps=1e-6, n_init = 10, check_loss = False, sample_weight = None):\n",
    "\n",
    "    clus_feats_np = clus_feats.detach().cpu().numpy()\n",
    "    best_k = min_clusters\n",
    "    best_score = -1\n",
    "    best_centers = None\n",
    "    \n",
    "    for k in range(min_clusters, min(max_clusters, clus_feats.shape[0]) + 1):\n",
    "            if k == 1:\n",
    "                centers = clus_feats.mean(dim=0, keepdim=True)\n",
    "                score = -1\n",
    "                labels = np.zeros(clus_feats.shape[0])\n",
    "            else:\n",
    "                model = KMeans(n_clusters=k, n_init=n_init, random_state=42)\n",
    "                with warnings.catch_warnings(record=True) as w:\n",
    "                    warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                    labels = model.fit_predict(clus_feats_np, sample_weight=sample_weight)\n",
    "                    if -1 in labels:\n",
    "                        print('exist -1')\n",
    "                    if len(set(labels)) < k:\n",
    "                        score = -1\n",
    "                    else:\n",
    "                        try:\n",
    "                            score = silhouette_score(clus_feats_np, labels)\n",
    "                        except:\n",
    "                            score = -1\n",
    "                centers = torch.tensor(model.cluster_centers_).cuda()\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_k = k\n",
    "                best_score = score\n",
    "                best_centers = centers\n",
    "                best_labels = labels\n",
    "\n",
    "    if check_loss:\n",
    "        return best_k, best_centers, best_score, (clus_feats * torch.nn.functional.normalize(best_centers[best_labels], dim = -1, p = 2)).sum(-1).mean()\n",
    "    \n",
    "    return best_k, best_centers, best_score, None\n",
    "\n",
    "\n",
    "# K-MEANS for cluster features\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "with torch.no_grad():\n",
    "    multi_lvl_cluster_features = []\n",
    "    multi_lvl_cluster_feature_weights = []\n",
    "    multi_lvl_cluster_feature_weights_only_direction = []\n",
    "\n",
    "    for lvl in range(NUM_LVL):\n",
    "        tmp_lvl_cluster_labels = multi_lvl_cluster_labels[lvl]\n",
    "        tmp_lvl_cluster_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "        print(len(tmp_lvl_cluster_labels), tmp_lvl_cluster_features.shape)\n",
    "\n",
    "    num_per_cluster_features = 20\n",
    "    for lvl in range(0,NUM_LVL):\n",
    "        cluster_to_masks = multi_lvl_cluster_to_masks[lvl]\n",
    "        flatten_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "\n",
    "        cluster_feature_loss = []\n",
    "        cluster_features = []\n",
    "        cluster_weights = []\n",
    "        cluster_weights_only_direction = []\n",
    "        for clus in tqdm(cluster_to_masks):\n",
    "\n",
    "            if clus.nonzero().shape[0] == 0:\n",
    "                cluster_features.append(torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda())\n",
    "                cluster_weights.append(torch.zeros(num_per_cluster_features).cuda())\n",
    "            else:\n",
    "                clus_feats = torch.nn.functional.normalize(flatten_features[clus.nonzero()].squeeze(1), dim = -1, p = 2).cuda()\n",
    "                avg_clus_feats = clus_feats.mean(dim = 0)\n",
    "                avg_clus_feats = torch.nn.functional.normalize(avg_clus_feats, dim = -1, p = 2)\n",
    "\n",
    "                best_k, cluster_centers, best_score, cl = dynamic_k_selection(clus_feats, max_clusters=min(clus_feats.shape[0], num_per_cluster_features), check_loss=True, sample_weight = None, n_init = 10)\n",
    "\n",
    "                num_centers = len(cluster_centers)\n",
    "\n",
    "                tmp_features = torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda()\n",
    "                tmp_features[:num_centers] = cluster_centers[:num_centers]\n",
    "\n",
    "                tmp_weights = torch.zeros(num_per_cluster_features).cuda()\n",
    "                tmp_weights_only_direction = torch.zeros(num_per_cluster_features).cuda()\n",
    "\n",
    "                t = torch.einsum('c,nc->n', avg_clus_feats.cuda(), tmp_features[:num_centers])\n",
    "\n",
    "                # In case you want to disable the internal compactness term of the weighted descriptor relevance aggregation (DW^c, see Table 4 and Sec. 6.4).\n",
    "                t2 = torch.einsum('c,nc->n', avg_clus_feats.cuda(), torch.nn.functional.normalize(tmp_features[:num_centers], dim = -1, p = 2))\n",
    "\n",
    "                tmp_weights[:num_centers] = t\n",
    "                tmp_weights_only_direction[:num_centers] = t2\n",
    "\n",
    "\n",
    "                cluster_features.append(tmp_features)\n",
    "                cluster_weights.append(tmp_weights)\n",
    "                cluster_weights_only_direction.append(tmp_weights_only_direction)\n",
    "\n",
    "\n",
    "\n",
    "        # Num_clusters * num_per_cluster_features, 512\n",
    "        cluster_features = torch.cat(cluster_features, 0)\n",
    "        cluster_weights = torch.cat(cluster_weights, 0)\n",
    "        cluster_weights_only_direction = torch.cat(cluster_weights_only_direction, 0)\n",
    "\n",
    "        multi_lvl_cluster_features.append(cluster_features)\n",
    "        multi_lvl_cluster_feature_weights.append(cluster_weights)\n",
    "        multi_lvl_cluster_feature_weights_only_direction.append(cluster_weights_only_direction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd8308",
   "metadata": {},
   "source": [
    "## Cross-View Descriptor Extraction: Dynamic K-Means with in loop denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5929a68",
   "metadata": {},
   "source": [
    "This is an experimental version not mentioned in the paper. In this version, we remove some noisy outliers during the K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b410c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10782 torch.Size([10782, 512])\n",
      "8332 torch.Size([8332, 512])\n",
      "2219 torch.Size([2219, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347/347 [04:17<00:00,  1.35it/s]\n",
      "100%|██████████| 207/207 [02:29<00:00,  1.39it/s]\n",
      "100%|██████████| 70/70 [00:54<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# K-Means with dynamic K selection\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "def dynamic_k_selection(clus_feats, min_clusters=1, max_clusters=20, eps=1e-6, n_init = 10):\n",
    "\n",
    "    clus_feats_np = clus_feats.detach().cpu().numpy()\n",
    "    best_k = min_clusters\n",
    "    best_score = -1\n",
    "    best_centers = None\n",
    "    \n",
    "    for k in range(min_clusters, min(max_clusters, clus_feats.shape[0]) + 1):\n",
    "            if k == 1:\n",
    "                centers = clus_feats.mean(dim=0, keepdim=True)\n",
    "                score = -1\n",
    "                labels = np.zeros(clus_feats.shape[0])\n",
    "            else:\n",
    "                model = KMeans(n_clusters=k, n_init=n_init, random_state=42)\n",
    "                with warnings.catch_warnings(record=True) as w:\n",
    "                    warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                    labels = model.fit_predict(clus_feats_np)\n",
    "                    if -1 in labels:\n",
    "                        print('exist -1')\n",
    "                    if len(set(labels)) < k:\n",
    "                        score = -1\n",
    "                    else:\n",
    "                        try:\n",
    "                            score = silhouette_score(clus_feats_np, labels)\n",
    "                        except:\n",
    "                            score = -1\n",
    "                centers = torch.tensor(model.cluster_centers_).cuda()\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_k = k\n",
    "                best_score = score\n",
    "                best_centers = centers\n",
    "                best_labels = labels\n",
    "\n",
    "    return best_k, best_centers, best_score, (clus_feats * torch.nn.functional.normalize(best_centers[best_labels], dim = -1, p = 2)).sum(-1)\n",
    "\n",
    "\n",
    "# K-MEANS for cluster features\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "with torch.no_grad():\n",
    "    multi_lvl_cluster_feature_loss_dynamic_kmeans = []\n",
    "    multi_lvl_cluster_features = []\n",
    "    multi_lvl_cluster_feature_weights = []\n",
    "    multi_lvl_cluster_feature_weights_only_direction = []\n",
    "\n",
    "    for lvl in range(NUM_LVL):\n",
    "        tmp_lvl_cluster_labels = multi_lvl_cluster_labels[lvl]\n",
    "        tmp_lvl_cluster_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "        print(len(tmp_lvl_cluster_labels), tmp_lvl_cluster_features.shape)\n",
    "\n",
    "    num_per_cluster_features = 20\n",
    "    for lvl in range(0,NUM_LVL):\n",
    "        cluster_to_masks = multi_lvl_cluster_to_masks[lvl]\n",
    "        flatten_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "\n",
    "        cluster_features = []\n",
    "        cluster_weights = []\n",
    "        cluster_weights_only_direction = []\n",
    "        for clus in tqdm(cluster_to_masks):\n",
    "\n",
    "            if clus.nonzero().shape[0] == 0:\n",
    "                cluster_features.append(torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda())\n",
    "                cluster_weights.append(torch.zeros(num_per_cluster_features).cuda())\n",
    "            else:\n",
    "                clus_feats = torch.nn.functional.normalize(flatten_features[clus.nonzero()].squeeze(1), dim = -1, p = 2).cuda()\n",
    "                avg_clus_feats = clus_feats.mean(dim = 0)\n",
    "                avg_clus_feats = torch.nn.functional.normalize(avg_clus_feats, dim = -1, p = 2)\n",
    "\n",
    "                available_clus_feat_mask = torch.ones(clus_feats.shape[0]).cuda()\n",
    "\n",
    "                # remove noise\n",
    "                while True:\n",
    "                    in_clus_feats = clus_feats[available_clus_feat_mask == 1]\n",
    "                    best_k, cluster_centers, best_score, clus_feature_similarities = dynamic_k_selection(in_clus_feats, max_clusters=min(in_clus_feats.shape[0], num_per_cluster_features), n_init = 10)\n",
    "                    if torch.all(clus_feature_similarities > 0.7):\n",
    "                        break\n",
    "                    else:\n",
    "                        available_clus_feat_mask[available_clus_feat_mask == 1] = (clus_feature_similarities > 0.7).float()\n",
    "\n",
    "                num_centers = len(cluster_centers)\n",
    "\n",
    "                tmp_features = torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda()\n",
    "                tmp_features[:num_centers] = cluster_centers[:num_centers]\n",
    "\n",
    "                tmp_weights = torch.zeros(num_per_cluster_features).cuda()\n",
    "                tmp_weights_only_direction = torch.zeros(num_per_cluster_features).cuda()\n",
    "\n",
    "                t = torch.einsum('c,nc->n', avg_clus_feats.cuda(), tmp_features[:num_centers])\n",
    "                t2 = torch.einsum('c,nc->n', avg_clus_feats.cuda(), torch.nn.functional.normalize(tmp_features[:num_centers], dim = -1, p = 2))\n",
    "\n",
    "                tmp_weights[:num_centers] = t\n",
    "                tmp_weights_only_direction[:num_centers] = t2\n",
    "\n",
    "\n",
    "                cluster_features.append(tmp_features)\n",
    "                cluster_weights.append(tmp_weights)\n",
    "                cluster_weights_only_direction.append(tmp_weights_only_direction)\n",
    "\n",
    "        # Num_clusters * num_per_cluster_features, 512\n",
    "        cluster_features = torch.cat(cluster_features, 0)\n",
    "        cluster_weights = torch.cat(cluster_weights, 0)\n",
    "        cluster_weights_only_direction = torch.cat(cluster_weights_only_direction, 0)\n",
    "\n",
    "        multi_lvl_cluster_features.append(cluster_features)\n",
    "        multi_lvl_cluster_feature_weights.append(cluster_weights)\n",
    "        multi_lvl_cluster_feature_weights_only_direction.append(cluster_weights_only_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a1e66",
   "metadata": {},
   "source": [
    "## Cross-View Descriptor Extraction: Naive K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11210 torch.Size([11210, 512])\n",
      "7139 torch.Size([7139, 512])\n",
      "3810 torch.Size([3810, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:12<00:00, 35.35it/s]\n",
      "100%|██████████| 224/224 [00:06<00:00, 34.63it/s]\n",
      "100%|██████████| 97/97 [00:03<00:00, 25.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# K-MEANS for cluster features\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "with torch.no_grad():\n",
    "    multi_lvl_cluster_features = []\n",
    "    multi_lvl_cluster_feature_weights = []\n",
    "\n",
    "    for lvl in range(NUM_LVL):\n",
    "        tmp_lvl_cluster_labels = multi_lvl_cluster_labels[lvl]\n",
    "        tmp_lvl_cluster_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "        print(len(tmp_lvl_cluster_labels), tmp_lvl_cluster_features.shape)\n",
    "\n",
    "    num_per_cluster_features = 20\n",
    "    for lvl in range(0,NUM_LVL):\n",
    "        cluster_to_masks = multi_lvl_cluster_to_masks[lvl]\n",
    "        flatten_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "\n",
    "        cluster_features = []\n",
    "        cluster_weights = []\n",
    "        for clus in tqdm(cluster_to_masks):\n",
    "\n",
    "            if clus.nonzero().shape[0] == 0:\n",
    "                cluster_features.append(torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda())\n",
    "                cluster_weights.append(torch.zeros(num_per_cluster_features).cuda())\n",
    "            else:\n",
    "                clus_feats = torch.nn.functional.normalize(flatten_features[clus.nonzero()].squeeze(1), dim = -1, p = 2).cuda()\n",
    "                avg_clus_feats = clus_feats.mean(dim = 0)\n",
    "                avg_clus_feats = torch.nn.functional.normalize(avg_clus_feats, dim = -1, p = 2)\n",
    "\n",
    "                if clus_feats.shape[0] > num_per_cluster_features:\n",
    "                    model = KMeans(n_clusters=num_per_cluster_features)\n",
    "\n",
    "                    with warnings.catch_warnings(record=True) as w:\n",
    "                        warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "        \n",
    "                        labels = model.fit_predict(clus_feats.detach().cpu().numpy())\n",
    "                        cluster_centers = torch.tensor(model.cluster_centers_).cuda()\n",
    "\n",
    "                        rep_center = False\n",
    "                        for warning in w:\n",
    "                            rep_center = rep_center or issubclass(warning.category, ConvergenceWarning)\n",
    "                            if rep_center: break\n",
    "                        \n",
    "                        if rep_center:\n",
    "                            similarity_matrix = cluster_centers @ cluster_centers.T\n",
    "\n",
    "                            threshold = 0.999\n",
    "                            duplicates = (similarity_matrix > threshold).float().triu(diagonal=1)\n",
    "                            duplicate_indices = torch.nonzero(duplicates, as_tuple=False)\n",
    "                            for idx in duplicate_indices:\n",
    "                                duplicate_center = idx[1].item()\n",
    "                                cluster_centers[duplicate_center] = torch.rand(flatten_features.shape[-1], device='cuda')\n",
    "\n",
    "                    num_centers = len(cluster_centers)\n",
    "                    tmp_features = torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda()\n",
    "                    tmp_features[:num_centers] = cluster_centers[:num_centers]\n",
    "\n",
    "                    tmp_weights = torch.zeros(num_per_cluster_features).cuda()\n",
    "                    t = torch.einsum('c,nc->n', avg_clus_feats.cuda(), tmp_features[:num_centers])\n",
    "\n",
    "                    tmp_weights[:num_centers] = t\n",
    "\n",
    "                    cluster_features.append(tmp_features)\n",
    "                    cluster_weights.append(tmp_weights)\n",
    "                else:\n",
    "                    tmp_features = torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda()\n",
    "                    tmp_features[:len(clus_feats)] = clus_feats\n",
    "\n",
    "                    tmp_weights = torch.zeros(num_per_cluster_features).cuda()\n",
    "                \n",
    "                    t = torch.einsum('c,nc->n', avg_clus_feats.cuda(), clus_feats)\n",
    "\n",
    "                    tmp_weights[:len(clus_feats)] = t\n",
    "\n",
    "                    cluster_features.append(tmp_features)\n",
    "                    cluster_weights.append(tmp_weights)\n",
    "                \n",
    "        # Num_clusters * 10, 512\n",
    "        cluster_features = torch.cat(cluster_features, 0)\n",
    "        cluster_weights = torch.cat(cluster_weights, 0)\n",
    "\n",
    "        cluster_features = torch.nn.functional.normalize(cluster_features, dim=-1, p=2)\n",
    "        multi_lvl_cluster_features.append(cluster_features)\n",
    "        multi_lvl_cluster_feature_weights.append(cluster_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3719e",
   "metadata": {},
   "source": [
    "## Cross-View Descriptor Extraction: Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6802b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11210 torch.Size([11210, 512])\n",
      "7139 torch.Size([7139, 512])\n",
      "3810 torch.Size([3810, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:00<00:00, 1530.32it/s]\n",
      "100%|██████████| 224/224 [00:00<00:00, 1553.01it/s]\n",
      "100%|██████████| 97/97 [00:00<00:00, 1604.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directly use average feature as a cluster center\n",
    "multi_lvl_cluster_features = []\n",
    "multi_lvl_cluster_feature_weights = []\n",
    "\n",
    "for lvl in range(3):\n",
    "    tmp_lvl_cluster_labels = multi_lvl_cluster_labels[lvl]\n",
    "    tmp_lvl_cluster_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "    print(len(tmp_lvl_cluster_labels), tmp_lvl_cluster_features.shape)\n",
    "\n",
    "num_per_cluster_features = 1\n",
    "for lvl in range(0,NUM_LVL):\n",
    "    cluster_to_masks = multi_lvl_cluster_to_masks[lvl]\n",
    "    flatten_features = multi_lvl_flatten_features[lvl][multi_lvl_mask_filter[lvl]]\n",
    "\n",
    "    cluster_feature_loss = []\n",
    "    retained_num = []\n",
    "    cluster_features = []\n",
    "    cluster_weights = []\n",
    "    inner_cosine = []\n",
    "    for clus in tqdm(cluster_to_masks):\n",
    "\n",
    "        if clus.nonzero().shape[0] == 0:\n",
    "            cluster_features.append(torch.rand(num_per_cluster_features, flatten_features.shape[-1]).cuda())\n",
    "            cluster_weights.append(torch.zeros(num_per_cluster_features).cuda())\n",
    "        else:\n",
    "            clus_feats = torch.nn.functional.normalize(flatten_features[clus.nonzero()].squeeze(1), dim = -1, p = 2).cuda()\n",
    "            avg_clus_feats = clus_feats.mean(dim = 0, keepdim=True)\n",
    "            avg_clus_feats = torch.nn.functional.normalize(avg_clus_feats, dim = -1, p = 2)\n",
    "\n",
    "\n",
    "            cluster_feature_loss.append((clus_feats * avg_clus_feats).sum(dim = -1).mean())\n",
    "\n",
    "            num_features = clus_feats.shape[0]\n",
    "            pos_features = (clus_feats * avg_clus_feats).sum(dim = -1) > 0.9\n",
    "            retained_num.append(pos_features.sum().item() / num_features)\n",
    "\n",
    "            cluster_features.append(avg_clus_feats)\n",
    "            cluster_weights.append(torch.tensor([1.]).cuda())\n",
    "\n",
    "            inner_cosine.append(torch.einsum('nc,kc->nk', clus_feats, clus_feats))\n",
    "\n",
    "    cluster_features = torch.cat(cluster_features, 0)\n",
    "    cluster_weights = torch.cat(cluster_weights, 0)\n",
    "    \n",
    "    cluster_features = torch.nn.functional.normalize(cluster_features, dim=-1, p=2)\n",
    "    multi_lvl_cluster_features.append(cluster_features)\n",
    "    multi_lvl_cluster_feature_weights.append(cluster_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the decomposition results and semantic descriptors\n",
    "\n",
    "torch.save(multi_lvl_cluster_features, os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_cluster_features.pth'))\n",
    "torch.save(multi_lvl_cluster_feature_weights, os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_cluster_feature_weights.pth'))\n",
    "torch.save(multi_lvl_seg_scores, os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_seg_scores.pth'))\n",
    "torch.save(multi_lvl_cluster_feature_weights_only_direction, os.path.join(os.path.dirname(FEATURE_PCD_PATH), 'multi_lvl_cluster_feature_weights_only_direction.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2502b3",
   "metadata": {},
   "source": [
    "# Language-driven Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee6d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension 512\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import clip_utils\n",
    "importlib.reload(clip_utils)\n",
    "from clip_utils import get_relevancy_cosine\n",
    "from clip_utils.clip_utils import load_clip, OpenCLIPNetworkConfig\n",
    "\n",
    "clip_config = OpenCLIPNetworkConfig()\n",
    "clip_config.clip_model_pretrained = CLIP_PATH\n",
    "\n",
    "clip_model = load_clip(clip_config)\n",
    "clip_model.eval()\n",
    "1\n",
    "\n",
    "from pytorch3d.ops import knn_points\n",
    "\n",
    "def bilateral_filter_with_color(points, scores, colors, K=16, neighbor_map = None, \n",
    "                                spatial_sigma=0.1, range_score_sigma=0.1, \n",
    "                                range_color_sigma=0.1):\n",
    "\n",
    "    N, _ = points.shape\n",
    "\n",
    "    if neighbor_map is None:\n",
    "        knn = knn_points(points[None, ...], points[None, ...], K=K, return_nn=False)\n",
    "        knn_idx = knn.idx[0]  # (N, K)\n",
    "    else:\n",
    "        knn_idx = neighbor_map\n",
    "\n",
    "    neighbor_points = points[knn_idx]       # (N, K, 3)\n",
    "    neighbor_scores = scores[knn_idx]       # (N, K)\n",
    "    neighbor_colors = colors[knn_idx]       # (N, K, 3)\n",
    "\n",
    "    points_expanded = points.unsqueeze(1).expand(-1, K, -1)   # (N, K, 3)\n",
    "    scores_expanded = scores.unsqueeze(1).expand(-1, K)       # (N, K)\n",
    "    colors_expanded = colors.unsqueeze(1).expand(-1, K, -1)   # (N, K, 3)\n",
    "\n",
    "    spatial_distance = torch.norm(neighbor_points - points_expanded, dim=2)  # (N, K)\n",
    "\n",
    "    spatial_weight = torch.exp(- (spatial_distance ** 2) / (2 * spatial_sigma ** 2))  # (N, K)\n",
    "\n",
    "    score_difference = scores_expanded - neighbor_scores  # (N, K)\n",
    "\n",
    "    range_score_weight = torch.exp(- (score_difference ** 2) / (2 * range_score_sigma ** 2))  # (N, K)\n",
    "\n",
    "    color_difference = torch.norm(neighbor_colors - colors_expanded, dim=2)  # (N, K)\n",
    "\n",
    "    range_color_weight = torch.exp(- (color_difference ** 2) / (2 * range_color_sigma ** 2))  # (N, K)\n",
    "\n",
    "    range_weight = range_score_weight * range_color_weight  # (N, K)\n",
    "\n",
    "    weights = spatial_weight * range_weight  # (N, K)\n",
    "\n",
    "    weights_normalized = weights / (torch.sum(weights, dim=1, keepdim=True) + 1e-8)  # (N, K)\n",
    "\n",
    "    filtered_scores = torch.sum(weights_normalized * neighbor_scores, dim=1)  # (N,)\n",
    "\n",
    "    return filtered_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7061491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all scene can apply this. Some scenes are too large.\n",
    "postprocess = True\n",
    "\n",
    "if postprocess:\n",
    "\n",
    "    from utils.sh_utils import SH2RGB\n",
    "\n",
    "    gaussian_colors = scene_gaussians._features_dc\n",
    "    gaussian_colors = SH2RGB(gaussian_colors.squeeze())\n",
    "    gaussian_colors = torch.clip(gaussian_colors, 0, 1)\n",
    "\n",
    "    K = 16\n",
    "    points = feature_gaussians.get_xyz\n",
    "    knn = knn_points(points[None, ...], points[None, ...], K=K, return_nn=False)\n",
    "    neighbor_map = knn.idx[0]  # (N, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d600a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster FEATURE based code\n",
    "prompt = \"jake from the adventure time\"\n",
    "with torch.no_grad():\n",
    "\n",
    "    point_colors = None\n",
    "    stack_of_cosine = []\n",
    "\n",
    "    multi_lvl_cluster_scores = []\n",
    "    for lvl in range(0,NUM_LVL):\n",
    "\n",
    "        cluster_features = multi_lvl_cluster_features[lvl]\n",
    "        cluster_weights = multi_lvl_cluster_feature_weights[lvl].clone()\n",
    "        seg_score = multi_lvl_seg_scores[lvl]\n",
    "        rel, pos, neg = get_relevancy_cosine(clip_model, torch.nn.functional.normalize(cluster_features.cuda(), dim = -1, p = 2), prompt)\n",
    "        cluster_scores = (rel * cluster_weights).reshape([-1, num_per_cluster_features])\n",
    "\n",
    "        cluster_scores, index = cluster_scores.max(dim = 1)[0], cluster_scores.max(dim = 1)[1]\n",
    "\n",
    "        multi_lvl_cluster_scores.append(cluster_scores)\n",
    "\n",
    "        pos = pos.reshape([-1, num_per_cluster_features])\n",
    "        batch_indices = torch.arange(pos.shape[0]).to(pos.device)  # [batch_size]\n",
    "\n",
    "        selected_pos = pos[batch_indices, index]  # [batch_size]\n",
    "\n",
    "\n",
    "        stack_of_cosine.append(selected_pos[seg_score.argmax(dim = -1).cpu().numpy()])\n",
    "\n",
    "\n",
    "    for lvl in [0,1,2]:\n",
    "        seg_score = multi_lvl_seg_scores[lvl]\n",
    "        cluster_scores = multi_lvl_cluster_scores[lvl]\n",
    "\n",
    "        cluster_colors = np.array(cluster_scores.cpu())\n",
    "\n",
    "        cluster_colors[cluster_colors < 0] = 0\n",
    "\n",
    "        cluster_colors = np.expand_dims(cluster_colors, axis=1)\n",
    "        cur_lvl_point_colors = cluster_colors[seg_score.argmax(dim = -1).cpu().numpy()]\n",
    "        point_colors = cur_lvl_point_colors if point_colors is None else point_colors + cur_lvl_point_colors\n",
    "    \n",
    "    stack_of_cosine = torch.stack(stack_of_cosine, 0)\n",
    "    stack_of_cosine = stack_of_cosine.max(dim = 0)[0]\n",
    "\n",
    "    point_colors /= NUM_LVL\n",
    "    # remove too low scores before min-max normalization for stability\n",
    "    point_colors = np.clip(point_colors, np.quantile(point_colors, 0.25), 1e9)\n",
    "    point_colors = point_colors - point_colors.min()\n",
    "    point_colors = point_colors / point_colors.max()\n",
    "    point_colors[(stack_of_cosine < 0.23).cpu().numpy()] = 0\n",
    "\n",
    "    point_colors[point_colors < 0.6] = 0\n",
    "    point_colors[point_colors != 0] = 1\n",
    "\n",
    "    if not postprocess:\n",
    "        point_colors = point_colors.repeat(3, axis=1)\n",
    "    else:\n",
    "        point_colors = bilateral_filter_with_color(feature_gaussians.get_xyz, torch.from_numpy(point_colors).squeeze().cuda(), gaussian_colors, spatial_sigma=0.5, range_score_sigma = 500, range_color_sigma = 500, neighbor_map=neighbor_map)\n",
    "        point_colors = point_colors.cpu().unsqueeze(-1).numpy().repeat(3, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0fb59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    background = torch.tensor([0,0,0], dtype=torch.float32, device=\"cuda\")\n",
    "    view = scene.getTrainCameras()[0]\n",
    "    render_res = render(view, feature_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(point_colors).float().cuda(), override_mask=torch.from_numpy(point_colors).float().cuda()[:,0:1])['render']\n",
    "    fg = (render_res > 0.5).float().permute([1,2,0]).detach().cpu().numpy()\n",
    "    plt.imshow(fg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2406f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment or remove the target\n",
    "\n",
    "scene_gaussians.roll_back()\n",
    "# segment\n",
    "p_mask = point_colors[:,0] > 0.5\n",
    "# remove\n",
    "# p_mask = point_colors[:,0] < 0.5\n",
    "scene_gaussians.segment(torch.from_numpy(p_mask).cuda().bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203e7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bg_color = [1 for i in range(3)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_image = render(cameras[0], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered_image.permute([1,2,0]).detach().cpu().numpy())\n",
    "scene_gaussians.roll_back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ef621",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef9444",
   "metadata": {},
   "source": [
    "## Eval Lerf-OVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8aadd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from LangSplat\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=100):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='firebrick', marker='o',\n",
    "               s=marker_size, edgecolor='black', linewidth=2.5, alpha=1)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='o',\n",
    "               s=marker_size, edgecolor='black', linewidth=1.5, alpha=1)   \n",
    "\n",
    "\n",
    "def show_box(boxes, ax, color=None):\n",
    "    if type(color) == str and color == 'random':\n",
    "        color = np.random.random(3)\n",
    "    elif color is None:\n",
    "        color = 'black'\n",
    "    for box in boxes.reshape(-1, 4):\n",
    "        x0, y0 = box[0], box[1]\n",
    "        w, h = box[2] - box[0], box[3] - box[1]\n",
    "        ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=color, facecolor=(0,0,0,0), lw=4, \n",
    "                                   capstyle='round', joinstyle='round', linestyle='dotted')) \n",
    "\n",
    "\n",
    "def show_result(image, point, bbox, save_path):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    rect = patches.Rectangle((0, 0), image.shape[1]-1, image.shape[0]-1, linewidth=0, edgecolor='none', facecolor='white', alpha=0.3)\n",
    "    plt.gca().add_patch(rect)\n",
    "    input_point = point.reshape(1,-1)\n",
    "    input_label = np.array([1])\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    show_box(bbox, plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0.0, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def smooth(mask):\n",
    "    h, w = mask.shape[:2]\n",
    "    im_smooth = mask.copy()\n",
    "    scale = 3\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            square = mask[max(0, i-scale) : min(i+scale+1, h-1),\n",
    "                          max(0, j-scale) : min(j+scale+1, w-1)]\n",
    "            im_smooth[i, j] = np.argmax(np.bincount(square.reshape(-1)))\n",
    "    return im_smooth\n",
    "\n",
    "def colormap_saving(image: torch.Tensor, save_path=None, colormap: str = \"turbo\", normalize: bool = True, colormap_min: float = -1.0, colormap_max: float = 1.0):\n",
    "    \"\"\"\n",
    "    Apply a colormap to an image tensor and save the result.\n",
    "    \n",
    "    If the image's shape is (h, w, 1): draw a colored relevance map.\n",
    "    If the image's shape is (h, w, 3): return it directly.\n",
    "    If the image's shape is (h, w, c): execute PCA and transform it into (h, w, 3).\n",
    "    \"\"\"\n",
    "    image_np = image.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Normalize the image if required\n",
    "    if normalize:\n",
    "        image_np = (image_np - colormap_min) / (colormap_max - colormap_min)\n",
    "        image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "    # Apply colormap\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    if image_np.ndim == 2 or (image_np.ndim == 3 and image_np.shape[-1] == 1):  # Single channel\n",
    "        output_image = cmap(image_np)[..., :3]  # Apply colormap and remove alpha channel\n",
    "    elif image_np.ndim == 3 and image_np.shape[-1] == 3:  # RGB image\n",
    "        output_image = image_np  # Directly use the RGB image\n",
    "    else:  # Multichannel image, reduce to 3 channels using PCA\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        image_flat = image_np.reshape(-1, image_np.shape[-1])\n",
    "        image_pca = pca.fit_transform(image_flat).reshape(image_np.shape[0], image_np.shape[1], 3)\n",
    "        output_image = (image_pca - np.min(image_pca)) / (np.max(image_pca) - np.min(image_pca))\n",
    "\n",
    "    # Save the image if save_path is provided\n",
    "    if save_path is not None:\n",
    "        plt.imsave(save_path.with_suffix(\".png\"), output_image)\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "def vis_mask_save(mask, save_path: Path = None):\n",
    "    mask_save = mask.copy()\n",
    "    mask_save[mask == 1] = 255\n",
    "    save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    cv2.imwrite(str(save_path), mask_save)\n",
    "\n",
    "\n",
    "def polygon_to_mask(img_shape, points_list):\n",
    "    points = np.asarray(points_list, dtype=np.int32)\n",
    "    mask = np.zeros(img_shape, dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [points], 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def stack_mask(mask_base, mask_add):\n",
    "    mask = mask_base.copy()\n",
    "    mask[mask_add != 0] = 1\n",
    "    return mask\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Union\n",
    "import numpy as np\n",
    "\n",
    "def eval_gt_lerfdata(json_folder: Union[str, Path] = None, ouput_path: Path = None):\n",
    "    \"\"\"\n",
    "    Organize LERF's gt annotations.\n",
    "    gt format:\n",
    "        file name: frame_xxxxx.json\n",
    "        file content: labelme format\n",
    "    return:\n",
    "        gt_ann: dict()\n",
    "            keys: str(int(idx))\n",
    "            values: dict()\n",
    "                keys: str(label)\n",
    "                values: dict() which contain 'bboxes' and 'mask'\n",
    "        dimensions: tuple (height, width)\n",
    "        img_paths: list of image file paths\n",
    "        idx_list: list of indices\n",
    "    \"\"\"\n",
    "    gt_json_paths = sorted(glob.glob(os.path.join(str(json_folder), 'frame_*.json')))\n",
    "    img_paths = sorted(glob.glob(os.path.join(str(json_folder), 'frame_*.jpg')))\n",
    "    gt_ann = {}\n",
    "    idx_list = []\n",
    "    \n",
    "    for js_path in gt_json_paths:\n",
    "        img_ann = defaultdict(dict)\n",
    "        with open(js_path, 'r') as f:\n",
    "            gt_data = json.load(f)\n",
    "        \n",
    "        h, w = gt_data['info']['height'], gt_data['info']['width']\n",
    "        idx = int(gt_data['info']['name'].split('_')[-1].split('.jpg')[0]) - 1\n",
    "        idx_list.append(idx)\n",
    "        \n",
    "        for prompt_data in gt_data[\"objects\"]:\n",
    "            label = prompt_data['category']\n",
    "            box = np.asarray(prompt_data['bbox']).reshape(-1)           # x1y1x2y2\n",
    "            mask = polygon_to_mask((h, w), prompt_data['segmentation'])\n",
    "            if img_ann[label].get('mask', None) is not None:\n",
    "                mask = stack_mask(img_ann[label]['mask'], mask)\n",
    "                img_ann[label]['bboxes'] = np.concatenate(\n",
    "                    [img_ann[label]['bboxes'].reshape(-1, 4), box.reshape(-1, 4)], axis=0)\n",
    "            else:\n",
    "                img_ann[label]['bboxes'] = box\n",
    "            img_ann[label]['mask'] = mask\n",
    "            \n",
    "            # Save for visualization\n",
    "            save_path = ouput_path / 'gt' / gt_data['info']['name'].split('.jpg')[0] / f'{label}.jpg'\n",
    "            save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "            vis_mask_save(mask, save_path)\n",
    "        \n",
    "        gt_ann[f'{idx}'] = img_ann\n",
    "\n",
    "    return gt_ann, (h, w), img_paths, idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d86aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "def smooth(mask: np.ndarray, kernel_size: int = 5) -> np.ndarray:\n",
    "    \"\"\"Apply a smoothing filter to the mask.\"\"\"\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size ** 2)\n",
    "    smoothed_mask = cv2.filter2D(mask.astype(np.float32), -1, kernel)\n",
    "    return np.clip(smoothed_mask, 0, 1).astype(np.uint8)\n",
    "\n",
    "def activate_stream(valid_map: torch.Tensor,\n",
    "                    image,\n",
    "                    img_ann: Dict = None, \n",
    "                    thresh: float = 0.5,\n",
    "                    image_name = None,\n",
    "                    show_results = True):\n",
    "\n",
    "    n_head, n_prompt, h, w = valid_map.shape\n",
    "    n_head = 1 # ignore the 2th and 3th layers\n",
    "    positives = list(img_ann.keys())\n",
    "\n",
    "    chosen_iou_list, chosen_lvl_list = [], []\n",
    "\n",
    "    for k in range(n_prompt):\n",
    "        iou_lvl = np.zeros(n_head)\n",
    "        mask_lvl = np.zeros((n_head, h, w))\n",
    "        for i in range(n_head):\n",
    "            \n",
    "            output_path_relev = image_name / 'heatmap' / f'{positives[k]}_{i}.png'\n",
    "            output_path_relev.parent.mkdir(exist_ok=True, parents=True)\n",
    "            if show_results:\n",
    "                colormap_saving(valid_map[i][k].unsqueeze(-1), output_path_relev)\n",
    "            \n",
    "            p_i = torch.clip(valid_map[i][k] - 0.5, 0, 1).unsqueeze(-1)\n",
    "            p_i_normalized = p_i / (p_i.max() + 1e-6)\n",
    "            cmap = plt.get_cmap(\"turbo\")\n",
    "            p_i_np = p_i_normalized.detach().cpu().numpy().squeeze()  # Remove single-dimensional entries\n",
    "            cmap = plt.get_cmap(\"turbo\")\n",
    "            p_i_colored = cmap(p_i_np)[..., :3]\n",
    "            valid_composited = torch.from_numpy(p_i_colored).to(p_i.device).float()\n",
    "\n",
    "            mask = (valid_map[i][k] < 0.5).squeeze()\n",
    "            valid_composited[mask, :] = image[mask, :] * 0.3\n",
    "            output_path_compo = image_name / 'composited' / f'{positives[k]}_{i}'\n",
    "            output_path_compo.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            if show_results:\n",
    "                colormap_saving(valid_composited, output_path_compo)\n",
    "            \n",
    "            # truncate the heatmap into mask\n",
    "            output = valid_map[i][k]\n",
    "\n",
    "            mask_pred = (output.detach().cpu().numpy() > thresh).astype(np.uint8)\n",
    "            mask_pred = smooth(mask_pred)\n",
    "            mask_lvl[i] = mask_pred\n",
    "            mask_gt = img_ann[positives[k]]['mask'].astype(np.uint8)\n",
    "            \n",
    "            # calculate iou\n",
    "            intersection = np.sum(np.logical_and(mask_gt, mask_pred))\n",
    "            union = np.sum(np.logical_or(mask_gt, mask_pred))\n",
    "            iou = np.sum(intersection) / np.sum(union)\n",
    "            iou_lvl[i] = iou\n",
    "\n",
    "        score_lvl = torch.zeros((n_head,), device=valid_map.device)\n",
    "        for i in range(n_head):\n",
    "            score = valid_map[i, k].max()\n",
    "            score_lvl[i] = score\n",
    "        chosen_lvl = torch.argmax(score_lvl)\n",
    "        \n",
    "        chosen_iou_list.append(iou_lvl[chosen_lvl])\n",
    "        chosen_lvl_list.append(chosen_lvl.detach().cpu().numpy())\n",
    "        \n",
    "        # save for visualization\n",
    "        save_path = image_name / f'chosen_{positives[k]}.png'\n",
    "        if show_results:\n",
    "            vis_mask_save(mask_lvl[chosen_lvl], save_path)\n",
    "\n",
    "    return chosen_iou_list, chosen_lvl_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624a3ab",
   "metadata": {},
   "source": [
    "## Eval on lerf_ovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8058de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder=\"data/lerf_ovs/label\"\n",
    "# \"figurines, waldo_kitchen, teatime, ramen\"\n",
    "dataset_name=\"figurines\"\n",
    "json_folder = os.path.join(json_folder, dataset_name)\n",
    "output_path = f\"output/eval_contra_result/{dataset_name}\"\n",
    "\n",
    "# Since the rendered 'mask' of 3D-GS is continuous, we need to set a threshold to convert it into binary mask.\n",
    "mask_thresh = 0.1\n",
    "\n",
    "point_thresh = 0.6\n",
    "cosine_thresh = 0.23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3598863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling\n",
    "@torch.no_grad()\n",
    "def cal_score_score_based(scene, clip_model, multi_lvl_cluster_to_masks, target_view, prompt = \"frog cup\"):\n",
    "    multi_lvl_rel_score = [[],[],[]]\n",
    "    with torch.no_grad():\n",
    "        for view in scene.getTrainCameras():\n",
    "            torch.cuda.empty_cache()\n",
    "            f = view.original_features.view(-1, 512)\n",
    "            f = torch.nn.functional.normalize(f.float(), dim = -1, p = 2)\n",
    "            rel, pos, neg = get_relevancy_cosine(clip_model, f.cuda(), prompt)\n",
    "\n",
    "            lvl_to_mask_id = []\n",
    "            lastone = -1\n",
    "            for i in range(0, 4):\n",
    "                tm = view.original_masks[i]\n",
    "                curone = tm.max().int().item()\n",
    "                lvl_to_mask_id.append([lastone+1, curone])\n",
    "                lastone = curone\n",
    "            \n",
    "            for lvl in range(3):\n",
    "                interval = lvl_to_mask_id[lvl+1]\n",
    "                tmp_rel = rel[interval[0]:interval[1]+1]\n",
    "                \n",
    "                multi_lvl_rel_score[lvl].append(tmp_rel)\n",
    "\n",
    "    for lvl in range(3):\n",
    "        multi_lvl_rel_score[lvl] = torch.cat(multi_lvl_rel_score[lvl], 0)\n",
    "\n",
    "\n",
    "    point_colors = None\n",
    "\n",
    "    for lvl in range(0,3):\n",
    "        selected_lvl = lvl\n",
    "        cluster_to_masks, rel_score = multi_lvl_cluster_to_masks[selected_lvl], multi_lvl_rel_score[selected_lvl][multi_lvl_mask_filter[lvl]]\n",
    "        seg_score = multi_lvl_seg_scores[selected_lvl]\n",
    "\n",
    "        cluster_to_masks = torch.stack(cluster_to_masks, 0)\n",
    "        cluster_scores = (cluster_to_masks * rel_score.unsqueeze(0)).max(dim = -1)[0]\n",
    "\n",
    "\n",
    "        cluster_colors = np.array(cluster_scores.cpu())\n",
    "\n",
    "        cluster_colors = np.expand_dims(cluster_colors, axis=1)\n",
    "\n",
    "        point_colors = cluster_colors[seg_score.argmax(dim = -1).cpu().numpy()] if point_colors is None else point_colors + cluster_colors[seg_score.argmax(dim = -1).cpu().numpy()]\n",
    "\n",
    "    point_colors /= 3\n",
    "\n",
    "    point_colors = np.clip(point_colors, np.quantile(point_colors, 0.25), 1e9)\n",
    "\n",
    "    point_colors = (point_colors - point_colors.min()) / (point_colors.max() - point_colors.min())\n",
    "\n",
    "    point_colors[point_colors < 0.6] = 0\n",
    "    point_colors[point_colors != 0] = 1\n",
    "\n",
    "    point_colors = bilateral_filter_with_color(feature_gaussians.get_xyz, torch.from_numpy(point_colors).squeeze().cuda(), gaussian_colors, spatial_sigma=0.5, range_score_sigma=0.5, range_color_sigma=0.5, neighbor_map=neighbor_map)\n",
    "\n",
    "    point_colors = point_colors.cpu().unsqueeze(-1).numpy().repeat(3, axis=1)\n",
    "\n",
    "    bg_color = [0 for i in range(3)]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    render_res = render(target_view, scene_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(point_colors).float().cuda())['render']\n",
    "    return render_res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def cal_score_feature_based(clip_model, multi_lvl_cluster_features, multi_lvl_cluster_feature_weights, num_per_cluster_features, target_view, prompt = \"frog cup\", stratey = \"mean\", thresh = point_thresh, cosine_thresh=cosine_thresh, clip_quantile = 0.25, neighbor_map = None, selected_lvls = None):\n",
    "    # point_colors = None\n",
    "    point_colors = []\n",
    "    stack_of_cosine = []\n",
    "    for lvl in range(3) if selected_lvls is None else selected_lvls:\n",
    "        cluster_features = multi_lvl_cluster_features[lvl]\n",
    "        cluster_features_weights = multi_lvl_cluster_feature_weights[lvl].clone()\n",
    "\n",
    "        # No reweighting (-DW)\n",
    "        # cluster_features_weights[cluster_features_weights != 0] = 1\n",
    "\n",
    "        # No Directional Consistency (+DW^C)\n",
    "        # cluster_features_weights[cluster_features_weights != 0] = cluster_features[cluster_features_weights != 0, :].norm(dim = -1)\n",
    "\n",
    "        seg_score = multi_lvl_seg_scores[lvl]\n",
    "        rel, pos, neg = get_relevancy_cosine(clip_model, torch.nn.functional.normalize(cluster_features.cuda(), dim = -1, p = 2), prompt)\n",
    "        # rel, pos, neg = get_relevancy_cosine(clip_model, cluster_features.cuda(), prompt)\n",
    "\n",
    "        cluster_scores = (rel * cluster_features_weights).reshape([-1, num_per_cluster_features])\n",
    "\n",
    "        cluster_scores, index = cluster_scores.max(dim = 1)[0], cluster_scores.max(dim = 1)[1]\n",
    "\n",
    "        pos = pos.reshape([-1, num_per_cluster_features])\n",
    "        batch_indices = torch.arange(pos.shape[0]).to(pos.device)  # [batch_size]\n",
    "        selected_pos = pos[batch_indices, index]  # [batch_size]\n",
    "        stack_of_cosine.append(selected_pos[seg_score.argmax(dim = -1).cpu().numpy()])\n",
    "\n",
    "\n",
    "        cluster_colors = np.array(cluster_scores.cpu())\n",
    "\n",
    "        cluster_colors[cluster_colors < 0] = 0\n",
    "        # unsqueeze numpy array\n",
    "        cluster_colors = np.expand_dims(cluster_colors, axis=1)\n",
    "        \n",
    "        cur_lvl_point_colors = cluster_colors[seg_score.argmax(dim = -1).cpu().numpy()]\n",
    "\n",
    "        point_colors.append(cur_lvl_point_colors)\n",
    "\n",
    "    if stratey == \"max\":\n",
    "        max_score = -1\n",
    "        max_idx = -1\n",
    "        for idx, point_color in enumerate(point_colors):\n",
    "            if point_color.max() > max_score:\n",
    "                max_score = point_color.max()\n",
    "                max_idx = idx\n",
    "        point_colors = point_colors[max_idx]\n",
    "        print(prompt, max_idx)\n",
    "    else:\n",
    "        point_colors = np.stack(point_colors, axis=0).mean(axis=0)\n",
    "    \n",
    "    stack_of_cosine = torch.stack(stack_of_cosine, 0)\n",
    "    stack_of_cosine = stack_of_cosine.max(dim = 0)[0]\n",
    "    cosine_filter = stack_of_cosine < cosine_thresh\n",
    "\n",
    "    point_colors = np.clip(point_colors, np.quantile(point_colors, clip_quantile), 1e9)\n",
    "\n",
    "    point_colors = (point_colors - point_colors.min()) / (point_colors.max() - point_colors.min())\n",
    "\n",
    "    point_colors[cosine_filter.cpu().numpy()] = 0\n",
    "\n",
    "    point_colors[point_colors < thresh] = 0\n",
    "    point_colors[point_colors != 0] = 1\n",
    "\n",
    "    point_colors = bilateral_filter_with_color(feature_gaussians.get_xyz, torch.from_numpy(point_colors).squeeze().cuda(), gaussian_colors, spatial_sigma=0.5, range_score_sigma=0.5, range_color_sigma=0.5, neighbor_map=neighbor_map)\n",
    "\n",
    "    point_colors = point_colors.cpu().unsqueeze(-1).numpy().repeat(3, axis=1)\n",
    "\n",
    "    bg_color = [0 for i in range(3)]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    render_res = render(target_view, feature_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(point_colors).float().cuda(), override_mask=torch.from_numpy(point_colors).float().cuda()[:,0:1])['render']\n",
    "\n",
    "    return render_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46f955",
   "metadata": {},
   "source": [
    "## EVAL LERF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c460cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 104, 151, 194]\n",
      "\u001b[91mold camera:0.192\u001b[0m, toy elephant:0.913, waldo:0.843, tesla door handle:0.594, porcelain hand:0.821, \u001b[91mrubber duck with hat:0.473\u001b[0m, \u001b[91mrubber duck with buoy:0.454\u001b[0m, pink ice cream:0.946, red toy chair:0.834, green apple:0.939, \u001b[91mpikachu:0.224\u001b[0m, \u001b[91mred apple:0.208\u001b[0m, spatula:0.83, jake:0.68, toy cat statue:0.625, pirate hat:0.827, miffy:0.912\n",
      "rubics cube:0.829, green apple:0.95, green toy chair:0.86, \u001b[91mjake:0.401\u001b[0m, \u001b[91mold camera:0.302\u001b[0m, pink ice cream:0.896, \u001b[91mpumpkin:0.0\u001b[0m, red apple:0.625, \u001b[91mrubber duck with hat:0.318\u001b[0m, \u001b[91mtesla door handle:0.388\u001b[0m, spatula:0.752, rubber duck with buoy:0.574, \u001b[91mpirate hat:0.154\u001b[0m\n",
      "rubber duck with hat:0.937, rubics cube:0.674, toy elephant:0.969, green apple:0.875, jake:0.557, toy cat statue:0.897, \u001b[91mpikachu:0.251\u001b[0m, porcelain hand:0.867, \u001b[91mred apple:0.151\u001b[0m, waldo:0.859, pirate hat:0.597\n",
      "toy elephant:0.911, pink ice cream:0.949, porcelain hand:0.883, green apple:0.911, green toy chair:0.972, \u001b[91mold camera:0.113\u001b[0m, rubics cube:0.901, spatula:0.812, toy cat statue:0.617, waldo:0.837, \u001b[91mrubber duck with buoy:0.368\u001b[0m, \u001b[91mpirate hat:0.281\u001b[0m, miffy:0.808, \u001b[91mbag:0.0\u001b[0m, rubber duck with hat:0.538\n",
      "Mean IoU: 0.6410\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SHOW_RESULTS = False\n",
    "\n",
    "new_iou_list = []\n",
    "per_class_iou_sum = {}\n",
    "per_class_iou_count = {}\n",
    "\n",
    "gt_ann, image_shape, image_paths, idx_list = eval_gt_lerfdata(Path(json_folder), Path(output_path))\n",
    "print(idx_list)\n",
    "chosen_iou_all, chosen_lvl_list = [], []\n",
    "acc_num = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    from utils.sh_utils import SH2RGB\n",
    "\n",
    "    gaussian_colors = scene_gaussians._features_dc\n",
    "    gaussian_colors = SH2RGB(gaussian_colors.squeeze())\n",
    "    gaussian_colors = torch.clip(gaussian_colors, 0, 1)\n",
    "\n",
    "    points_xyz = feature_gaussians.get_xyz\n",
    "    N, _ = points_xyz.shape\n",
    "    knn = knn_points(points_xyz[None, ...], points_xyz[None, ...], K=16, return_nn=False)\n",
    "    neighbor_map = knn.idx[0]  # (N, K)\n",
    "\n",
    "\n",
    "for j, idx in enumerate(idx_list):\n",
    "    rendered_similarities_list = []\n",
    "    idx_str = str(idx)\n",
    "    item_key_list = list(gt_ann[idx_str].keys())\n",
    "    img_ann = gt_ann[f'{idx}']\n",
    "    \n",
    "    for item_key in item_key_list:\n",
    "        gt_mask = gt_ann[idx_str][item_key]['mask']\n",
    "        gt_bbox = gt_ann[idx_str][item_key]['bboxes']\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Rendering\n",
    "        start = time.time()\n",
    "\n",
    "        rendered_similarities = cal_score_feature_based(clip_model, multi_lvl_cluster_features, multi_lvl_cluster_feature_weights, num_per_cluster_features, cameras[idx], item_key, stratey='mean', neighbor_map=neighbor_map)\n",
    "\n",
    "        # No internal compactness term\n",
    "        # rendered_similarities = cal_score_feature_based(clip_model, multi_lvl_cluster_features, multi_lvl_cluster_feature_weights_only_direction, num_per_cluster_features, cameras[idx], item_key, stratey='mean', neighbor_map=neighbor_map)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "\n",
    "        rendered_similarities_list.append(rendered_similarities)\n",
    "\n",
    "    rendered_similarities_stack = torch.stack(rendered_similarities_list, dim=1)\n",
    "\n",
    "    first_rendered_similarity = rendered_similarities_stack[0]\n",
    "    first_rendered_similarity_np = first_rendered_similarity.detach().cpu().numpy()\n",
    "    \n",
    "    image_name = Path(output_path) / f'{idx+1:0>5}'\n",
    "    image_name.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    rgb_img = cv2.imread(image_paths[j])[..., ::-1]\n",
    "    rgb_img = (rgb_img / 255.0).astype(np.float32)\n",
    "    rgb_img = torch.from_numpy(rgb_img).to(\"cuda\")\n",
    "    # Evaluating\n",
    "    c_iou_list, c_lvl = activate_stream(rendered_similarities_stack, rgb_img, img_ann, thresh=mask_thresh, image_name = image_name, show_results=SHOW_RESULTS)\n",
    "    chosen_iou_all.extend(c_iou_list)\n",
    "    chosen_lvl_list.extend(c_lvl)\n",
    "\n",
    "    # keep 3 decimal places\n",
    "    print_res = []\n",
    "    for a,b in zip(item_key_list, c_iou_list):\n",
    "        per_class_iou_sum[a] = per_class_iou_sum.get(a, 0) + b\n",
    "        per_class_iou_count[a] = per_class_iou_count.get(a, 0) + 1\n",
    "        if b > 0.5:\n",
    "            print_res.append(f'{a}:{round(b, 3)}')\n",
    "        else:\n",
    "            print_res.append(f'\\033[91m{a}:{round(b, 3)}\\033[0m')\n",
    "        \n",
    "        if a != 'pikachu' and a != 'pumpkin':\n",
    "            new_iou_list.append(b)\n",
    "    print_res = ', '.join(print_res)\n",
    "\n",
    "    print(print_res)\n",
    "\n",
    "# iou\n",
    "mean_iou = sum(chosen_iou_all) / len(chosen_iou_all)\n",
    "\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2841b",
   "metadata": {},
   "source": [
    "# Eval Scannet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu40_dict = {\n",
    "    0: \"unlabeled\", 1: \"wall\", 2: \"floor\", 3: \"cabinet\", 4: \"bed\", 5: \"chair\",\n",
    "    6: \"sofa\", 7: \"table\", 8: \"door\", 9: \"window\", 10: \"bookshelf\",\n",
    "    11: \"picture\", 12: \"counter\", 13: \"blinds\", 14: \"desk\", 15: \"shelves\",\n",
    "    16: \"curtain\", 17: \"dresser\", 18: \"pillow\", 19: \"mirror\", 20: \"floormat\",\n",
    "    21: \"clothes\", 22: \"ceiling\", 23: \"books\", 24: \"refrigerator\", 25: \"television\",\n",
    "    26: \"paper\", 27: \"towel\", 28: \"showercurtain\", 29: \"box\", 30: \"whiteboard\",\n",
    "    31: \"person\", 32: \"nightstand\", 33: \"toilet\", 34: \"sink\", 35: \"lamp\",\n",
    "    36: \"bathtub\", 37: \"bag\", 38: \"otherstructure\", 39: \"otherfurniture\", 40: \"otherprop\"\n",
    "}\n",
    "\n",
    "# ScanNet 20 classes\n",
    "scannet19_dict = {\n",
    "    1: \"wall\", 2: \"floor\", 3: \"cabinet\", 4: \"bed\", 5: \"chair\",\n",
    "    6: \"sofa\", 7: \"table\", 8: \"door\", 9: \"window\", 10: \"bookshelf\",\n",
    "    11: \"picture\", 12: \"counter\", 14: \"desk\", 16: \"curtain\",\n",
    "    24: \"refrigerator\", 28: \"shower curtain\", 33: \"toilet\", 34: \"sink\",\n",
    "    36: \"bathtub\", # 39: \"otherfurniture\"\n",
    "}\n",
    "\n",
    "def create_cityscapes_label_colormap(show_classid = -1):\n",
    "    \"\"\"Creates a label colormap used in CITYSCAPES segmentation benchmark.\n",
    "\n",
    "    Returns:\n",
    "        A colormap for visualizing segmentation results.\n",
    "    \"\"\"\n",
    "    colormap = np.zeros((256, 3), dtype=np.uint8)\n",
    "\n",
    "    if show_classid < 0:\n",
    "        colormap[0] = [0, 0, 0]\n",
    "        colormap[1] = [244, 35, 232]\n",
    "        colormap[2] = [70, 70, 70]\n",
    "        colormap[3] = [102, 102, 156]\n",
    "        colormap[4] = [190, 153, 153]\n",
    "        colormap[5] = [153, 153, 153]\n",
    "        colormap[6] = [250, 170, 30]\n",
    "        colormap[7] = [220, 220, 0]\n",
    "        colormap[8] = [107, 142, 35]\n",
    "        colormap[9] = [152, 251, 152]\n",
    "        colormap[10] = [70, 130, 180]\n",
    "        colormap[11] = [220, 20, 60]\n",
    "        colormap[12] = [255, 0, 0]\n",
    "        colormap[13] = [0, 0, 142]\n",
    "        colormap[14] = [0, 0, 70]\n",
    "        colormap[15] = [0, 60, 100]\n",
    "        colormap[16] = [0, 80, 100]\n",
    "        colormap[17] = [0, 0, 230]\n",
    "        colormap[18] = [119, 11, 32]\n",
    "    else:\n",
    "        colormap[show_classid] = [255, 0, 0]\n",
    "    return colormap\n",
    "\n",
    "\n",
    "def read_labels_from_ply(file_path):\n",
    "    ply_data = PlyData.read(file_path)\n",
    "    vertex_data = ply_data['vertex'].data\n",
    "    # Extract the coordinates and labels of the points. The labels are from 1 to 40 for the NYU40 dataset, with 0 being invalid.\n",
    "    points = np.vstack([vertex_data['x'], vertex_data['y'], vertex_data['z']]).T\n",
    "    labels = vertex_data['label']\n",
    "    return points, labels\n",
    "\n",
    "def calculate_metrics(gt, pred, total_classes):\n",
    "    gt = gt.cpu()\n",
    "    pred = pred.cpu()\n",
    "    pred[gt == 0] = 0\n",
    "\n",
    "    ious = torch.zeros(total_classes)\n",
    "\n",
    "    intersection = torch.zeros(total_classes)\n",
    "    union = torch.zeros(total_classes)\n",
    "    correct = torch.zeros(total_classes)\n",
    "    total = torch.zeros(total_classes)\n",
    "\n",
    "    for cls in range(1, total_classes):\n",
    "        intersection[cls] = torch.sum((gt == cls) & (pred == cls)).item()\n",
    "        union[cls] = torch.sum((gt == cls) | (pred == cls)).item()\n",
    "        correct[cls] = torch.sum((gt == cls) & (pred == cls)).item()\n",
    "        total[cls] = torch.sum(gt == cls).item()\n",
    "\n",
    "    valid_union = union != 0\n",
    "    ious[valid_union] = intersection[valid_union] / union[valid_union]\n",
    "\n",
    "    # Only consider the categories that exist in the current scene\n",
    "    gt_classes = torch.unique(gt)\n",
    "    valid_gt_classes = gt_classes[gt_classes != 0]  # ignore 0\n",
    "\n",
    "    # miou\n",
    "    mean_iou = ious[valid_gt_classes].mean().item()\n",
    "\n",
    "    # acc\n",
    "    valid_mask = gt != 0\n",
    "    correct_predictions = torch.sum((gt == pred) & valid_mask).item()\n",
    "    total_valid_points = torch.sum(valid_mask).item()\n",
    "    accuracy = correct_predictions / total_valid_points if total_valid_points > 0 else float('nan')\n",
    "\n",
    "    class_accuracy = correct / total\n",
    "    # mAcc.\n",
    "    mean_class_accuracy = class_accuracy[valid_gt_classes].mean().item()\n",
    "\n",
    "    return ious, mean_iou, accuracy, mean_class_accuracy\n",
    "\n",
    "def bilateral_filter_with_color_(points, scores, colors, K=16, neighbor_map = None, \n",
    "                                spatial_sigma=0.5, range_score_sigma=0.5, \n",
    "                                range_color_sigma=0.5):\n",
    "    N, _ = points.shape\n",
    "\n",
    "    if neighbor_map is None:\n",
    "        knn = knn_points(points[None, ...], points[None, ...], K=K, return_nn=False)\n",
    "        knn_idx = knn.idx[0]  # (N, K)\n",
    "    else:\n",
    "        knn_idx = neighbor_map\n",
    "    neighbor_points = points[knn_idx]       # (N, K, 3)\n",
    "    neighbor_scores = scores[knn_idx]       # (N, K, S)\n",
    "    neighbor_colors = colors[knn_idx]       # (N, K, 3)\n",
    "\n",
    "    points_expanded = points.unsqueeze(1).expand(-1, K, -1)   # (N, K, 3)\n",
    "    scores_expanded = scores.unsqueeze(1).expand(-1, K, -1)   # (N, K, S)\n",
    "    colors_expanded = colors.unsqueeze(1).expand(-1, K, -1)   # (N, K, 3)\n",
    "    spatial_distance = torch.norm(neighbor_points - points_expanded, dim=2)  # (N, K)\n",
    "    spatial_weight = torch.exp(- (spatial_distance ** 2) / (2 * spatial_sigma ** 2))  # (N, K)\n",
    "    score_difference = torch.norm(neighbor_scores - scores_expanded, dim=2)\n",
    "    range_score_weight = torch.exp(- (score_difference ** 2) / (2 * range_score_sigma ** 2))  # (N, K)\n",
    "    color_difference = torch.norm(neighbor_colors - colors_expanded, dim=2)  # (N, K)\n",
    "    range_color_weight = torch.exp(- (color_difference ** 2) / (2 * range_color_sigma ** 2))  # (N, K)\n",
    "    range_weight = range_score_weight * range_color_weight  # (N, K)\n",
    "    weights = spatial_weight * range_weight  # (N, K)\n",
    "    weights_normalized = weights / (torch.sum(weights, dim=1, keepdim=True) + 1e-8)  # (N, K)\n",
    "    filtered_scores = torch.sum(weights_normalized.unsqueeze(-1) * neighbor_scores, dim=1)  # (N, S)\n",
    "\n",
    "    return filtered_scores\n",
    "\n",
    "\n",
    "def get_point_seg(clip_model, multi_lvl_cluster_features, multi_lvl_cluster_feature_weights, label_list, cosine_thresh = 0.23, selected_lvls = None):\n",
    "    per_point_scores_for_all_labels = []\n",
    "    per_point_stack_of_cosine = []\n",
    "    for label in label_list:\n",
    "        # cluster FEATURE based code\n",
    "        point_scores = None\n",
    "        stack_of_cosine = []\n",
    "        for lvl in range(0,NUM_LVL) if selected_lvls is None else selected_lvls:\n",
    "            cluster_features = multi_lvl_cluster_features[lvl]\n",
    "            cluster_weights = multi_lvl_cluster_feature_weights[lvl]\n",
    "            seg_score = multi_lvl_seg_scores[lvl]\n",
    "            rel, pos, neg = get_relevancy_cosine(clip_model, cluster_features.cuda(), label)\n",
    "            cluster_scores = (rel.squeeze() * cluster_weights).reshape([-1, num_per_cluster_features])\n",
    "            cluster_scores, index = cluster_scores.max(dim = 1)[0], cluster_scores.max(dim = 1)[1]\n",
    "            # cluster_scores = cluster_scores.mean(dim = 1)\n",
    "            pos = pos.reshape([-1, num_per_cluster_features])\n",
    "            batch_indices = torch.arange(pos.shape[0]).to(pos.device)  # [batch_size]\n",
    "            selected_pos = pos[batch_indices, index]  # [batch_size]\n",
    "            stack_of_cosine.append(selected_pos[seg_score.argmax(dim = -1).cpu().numpy()])\n",
    "            \n",
    "            point_scores = cluster_scores[seg_score.argmax(dim = -1).cpu().numpy()] if point_scores is None else point_scores + cluster_scores[seg_score.argmax(dim = -1).cpu().numpy()]\n",
    "\n",
    "        # N_point * N_lvl -> N_point\n",
    "        stack_of_cosine = torch.stack(stack_of_cosine, -1).max(dim = -1)[0]\n",
    "\n",
    "\n",
    "        point_scores /= NUM_LVL\n",
    "        per_point_scores_for_all_labels.append(point_scores)\n",
    "\n",
    "        # N_labels, N_point\n",
    "        per_point_stack_of_cosine.append(stack_of_cosine)\n",
    "\n",
    "    per_point_stack_of_cosine = torch.stack(per_point_stack_of_cosine, 0).permute(1,0)\n",
    "    per_point_scores_for_all_labels = torch.stack(per_point_scores_for_all_labels, dim = -1)\n",
    "    \n",
    "    per_point_scores_for_all_labels[per_point_stack_of_cosine < cosine_thresh] = 0\n",
    "    return per_point_scores_for_all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0819396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_list = ['scene0000_00', 'scene0062_00', 'scene0070_00', 'scene0097_00', 'scene0140_00', 'scene0200_00', 'scene0347_00', 'scene0400_00', 'scene0590_00', 'scene0645_00']\n",
    "\n",
    "scan_name = scene_list[0]\n",
    "target_id = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36]   # 19\n",
    "target_id = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 33, 34]   # 15\n",
    "target_id = [1,2,4,5,6,7,8,9,10,33] # 10\n",
    "\n",
    "gt_file_path = f\"/data_hdd3/cenjiazhong/datasets/nerf_data/scannet/{scan_name}/{scan_name}_vh_clean_2.labels.ply\"\n",
    "\n",
    "target_dict = {key: nyu40_dict[key] for key in target_id}\n",
    "target_names = list(target_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gts\n",
    "# Obtained new point cloud labels, taking 19 categories as an example, where updated_labels are labels 0, 1-19.\n",
    "points, labels = read_labels_from_ply(gt_file_path)\n",
    "\n",
    "target_id_mapping = {value: index + 1 for index, value in enumerate(target_id)}\n",
    "updated_labels = np.zeros_like(labels)\n",
    "for original_value, new_value in target_id_mapping.items():\n",
    "    updated_labels[labels == original_value] = new_value\n",
    "updated_gt_labels = torch.from_numpy(updated_labels.astype(np.int64)).cuda()\n",
    "\n",
    "# NOTE Filter out points based on their opacity values.\n",
    "ignored_pts = feature_gaussians.get_opacity.squeeze() < 0.1\n",
    "updated_gt_labels[ignored_pts] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47899bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene: scene0000_00, mIoU: 0.4342, mAcc.: 0.6690\n",
      "wall: 0.4720\n",
      "floor: 0.5986\n",
      "bed: 0.5134\n",
      "chair: 0.0000\n",
      "sofa: 0.4724\n",
      "table: 0.3493\n",
      "door: 0.3328\n",
      "window: 0.2925\n",
      "bookshelf: 0.0000\n",
      "toilet: 0.4424\n"
     ]
    }
   ],
   "source": [
    "res = get_point_seg(clip_model, multi_lvl_cluster_features, multi_lvl_cluster_feature_weights, target_names, cosine_thresh=0.21)\n",
    "res = bilateral_filter_with_color_(feature_gaussians.get_xyz, res, gaussian_colors, spatial_sigma=0.5, range_score_sigma = 0.5, range_color_sigma = 0.5, neighbor_map=neighbor_map)\n",
    "\n",
    "pred_pts_score, pred_pts_cls_id = torch.max(res, dim=-1)\n",
    "pred_pts_cls_id[pred_pts_score == 0] = -1\n",
    "pred_pts_cls_id += 1\n",
    "\n",
    "\n",
    "ious, mean_iou, accuracy, mean_acc = calculate_metrics(updated_gt_labels, pred_pts_cls_id, total_classes=len(target_names)+1)\n",
    "print(f\"Scene: {scan_name}, mIoU: {mean_iou:.4f}, mAcc.: {mean_acc:.4f}\")\n",
    "\n",
    "for iou, tn in zip(ious[1:], target_names):\n",
    "    print(f\"{tn}: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a372b",
   "metadata": {},
   "source": [
    "# Eval 3D-OVS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c330b63",
   "metadata": {},
   "source": [
    "Before running this code, please check the 'Only Keep the \"Whole\" Layer Features and Masks (for 3D-OVS)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_3dovs_eval_data(root = './data/3dovs', scene = 'bed'):\n",
    "    mask_folder = os.path.join(root, scene, 'segmentations')\n",
    "\n",
    "    f = open(f'{mask_folder}/classes.txt', 'r')\n",
    "    \n",
    "    classes = [name.rstrip('\\n') for name in f.readlines()]\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    cam_names = [c for c in os.listdir(mask_folder) if not c.endswith('.txt')]\n",
    "    # subfolders = [os.path.join(mask_folder, c) for c in cam_names]\n",
    "    gt_imgs = {}\n",
    "\n",
    "    for cam_n in cam_names:\n",
    "        subfolder = os.path.join(mask_folder, cam_n)\n",
    "        gt_in_this_folder = {}\n",
    "\n",
    "        for file in os.listdir(subfolder):\n",
    "            if file.endswith('.png'):\n",
    "                tmp = plt.imread(os.path.join(subfolder, file))\n",
    "                tmp_gt = tmp[:,:,0]\n",
    "                gt_in_this_folder[file.split('.')[0]] = tmp_gt\n",
    "        gt_imgs[cam_n] = gt_in_this_folder\n",
    "\n",
    "    return gt_imgs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e35738",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_imgs, classes = load_3dovs_eval_data(scene='room')\n",
    "print(gt_imgs.keys(), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "for idx, cla in enumerate(classes):\n",
    "    seg_res = []\n",
    "    gts = []\n",
    "    for image_name in gt_imgs.keys():\n",
    "        if cla not in gt_imgs[image_name].keys():\n",
    "            continue\n",
    "        gts.append(torch.from_numpy(gt_imgs[image_name][cla]))\n",
    "        for cam in scene.getTrainCameras():\n",
    "            if cam.image_name == image_name:\n",
    "                break\n",
    "        res = cal_score_feature_based(scene, clip_model, multi_lvl_cluster_features, multi_lvl_cluster_feature_weights, num_per_cluster_features, cam, prompt = cla, stratey = \"mean\", thresh = 0.6, cosine_thresh=0.0, clip_quantile=0.0, neighbor_map = neighbor_map, selected_lvls=[0])\n",
    "\n",
    "        seg_res.append(res[0,:,:])\n",
    "\n",
    "    eval_results[cla] = (torch.stack(seg_res).cpu(), torch.stack(gts).cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mIoU = []\n",
    "for k in eval_results.keys():\n",
    "    upsampled_pred = torch.nn.functional.interpolate(eval_results[k][0].unsqueeze(1), size=eval_results[k][1].shape[-2:], mode='bilinear').squeeze() > 0.8\n",
    "    iou = (upsampled_pred & eval_results[k][1].bool()).sum() / (upsampled_pred | eval_results[k][1].bool()).sum()\n",
    "    print(f\"{k}:{iou}\")\n",
    "    mIoU.append(iou)\n",
    "print('mIoU:', sum(mIoU).item() / len(mIoU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f1e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fafb6fc07d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(torch.nn.functional.interpolate(eval_results['shrilling chicken'][0].unsqueeze(1), size=eval_results[k][1].shape[-2:], mode='bilinear').squeeze()[0] > 0.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(eval_results['shrilling chicken'][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81094382",
   "metadata": {},
   "source": [
    "# Only Keep the \"Whole\" Layer Features and Masks (for 3D-OVS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546200e",
   "metadata": {},
   "source": [
    "After extracting the masks and corresponding features with preprocess.py for 3D-OVS, we use this part of code to remove unwanted masks. To use this code, please modify the output path of preprocess.py from language_features to language_features_3dovs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79752e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_langsplat_feature_mask_to_opengaussian(langsplat_f_path = ''):\n",
    "    langsplat_f = np.load(langsplat_f_path)\n",
    "    langsplat_s = np.load(langsplat_f_path[:-5] + 's.npy')\n",
    "    final_layer_mask = langsplat_s[-1:]\n",
    "    final_layer_mask = np.repeat(final_layer_mask, 4, axis = 0)\n",
    "    print(np.unique(final_layer_mask))\n",
    "    f = int(final_layer_mask[final_layer_mask > -1].min())\n",
    "    t = int(final_layer_mask.max())\n",
    "\n",
    "    final_layer_mask -= f\n",
    "    final_layer_mask[final_layer_mask < 0] = -1\n",
    "\n",
    "    new_features = langsplat_f[f: t+1]\n",
    "    print(new_features.shape, langsplat_f.shape)\n",
    "\n",
    "    return final_layer_mask, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. 32. 33. 34. 35. 36.]\n",
      "(5, 512) (37, 512)\n",
      "[-1. 32. 33. 34. 35. 36. 37.]\n",
      "(6, 512) (38, 512)\n",
      "[-1. 37. 38. 39. 40. 41. 42.]\n",
      "(6, 512) (43, 512)\n",
      "[-1. 29. 30. 31. 32. 33.]\n",
      "(5, 512) (34, 512)\n",
      "[-1. 33. 34. 35. 36. 37. 38.]\n",
      "(6, 512) (39, 512)\n",
      "[-1. 42. 43. 44. 45. 46. 47.]\n",
      "(6, 512) (48, 512)\n",
      "[-1. 28. 29. 30.]\n",
      "(3, 512) (31, 512)\n",
      "[-1. 36. 37. 38. 39. 40.]\n",
      "(5, 512) (41, 512)\n",
      "[-1. 36. 37. 38. 39. 40.]\n",
      "(5, 512) (41, 512)\n",
      "[-1. 27. 28. 29. 30. 31.]\n",
      "(5, 512) (32, 512)\n",
      "[-1. 40. 41. 42. 43. 44. 45.]\n",
      "(6, 512) (46, 512)\n",
      "[-1. 41. 42. 43. 44. 45. 46.]\n",
      "(6, 512) (47, 512)\n",
      "[-1. 35. 36. 37. 38. 39. 40.]\n",
      "(6, 512) (41, 512)\n",
      "[-1. 40. 41. 42. 43. 44. 45.]\n",
      "(6, 512) (46, 512)\n",
      "[-1. 44. 45. 46. 47. 48. 49.]\n",
      "(6, 512) (50, 512)\n",
      "[-1. 27. 28. 29. 30. 31.]\n",
      "(5, 512) (32, 512)\n",
      "[-1. 38. 39. 40. 41. 42. 43.]\n",
      "(6, 512) (44, 512)\n",
      "[-1. 47. 48. 49. 50. 51. 52.]\n",
      "(6, 512) (53, 512)\n",
      "[-1. 29. 30. 31. 32. 33. 34.]\n",
      "(6, 512) (35, 512)\n",
      "[-1. 38. 39. 40. 41. 42.]\n",
      "(5, 512) (43, 512)\n",
      "[-1. 38. 39. 40. 41. 42. 43.]\n",
      "(6, 512) (44, 512)\n",
      "[-1. 44. 45. 46. 47. 48. 49.]\n",
      "(6, 512) (50, 512)\n",
      "[-1. 33. 34. 35. 36. 37.]\n",
      "(5, 512) (38, 512)\n",
      "[-1. 31. 32. 33. 34. 35.]\n",
      "(5, 512) (36, 512)\n",
      "[-1. 25. 26. 27. 28. 29.]\n",
      "(5, 512) (30, 512)\n",
      "[-1. 36. 37. 38. 39. 40. 41. 42.]\n",
      "(7, 512) (43, 512)\n",
      "[-1. 40. 41. 42. 43. 44. 45.]\n",
      "(6, 512) (46, 512)\n",
      "[-1. 39. 40. 41. 42. 43. 44.]\n",
      "(6, 512) (45, 512)\n",
      "[-1. 33. 34. 35. 36. 37. 38.]\n",
      "(6, 512) (39, 512)\n",
      "[-1. 25. 26. 27. 28.]\n",
      "(4, 512) (29, 512)\n",
      "[-1. 33. 34. 35. 36. 37. 38.]\n",
      "(6, 512) (39, 512)\n",
      "[-1. 24. 25. 26. 27. 28.]\n",
      "(5, 512) (29, 512)\n",
      "[-1. 45. 46. 47. 48. 49. 50.]\n",
      "(6, 512) (51, 512)\n",
      "[-1. 33. 34. 35. 36. 37. 38.]\n",
      "(6, 512) (39, 512)\n",
      "[-1. 33. 34. 35. 36. 37. 38.]\n",
      "(6, 512) (39, 512)\n",
      "[-1. 29. 30. 31. 32. 33.]\n",
      "(5, 512) (34, 512)\n",
      "[-1. 31. 32. 33. 34. 35. 36.]\n",
      "(6, 512) (37, 512)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "root = './data/3dovs/bed/'\n",
    "for fname in os.listdir(os.path.join(root, 'language_features_3dovs')):\n",
    "    if not fname.endswith('_f.npy'):\n",
    "        continue\n",
    "    m, f = convert_langsplat_feature_mask_to_opengaussian(os.path.join(root,'language_features_3dovs',fname))\n",
    "    np.save(os.path.join(root,'language_features',fname[:-5] + 'f.npy'), f)\n",
    "    np.save(os.path.join(root,'language_features',fname[:-5] + 's.npy'), m)\n",
    "\n",
    "# sofa bench"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
